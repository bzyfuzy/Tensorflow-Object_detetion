{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {\n",
    "    'protobuff': \"https://github.com/protocolbuffers/protobuf/releases/download/v25.0-rc1/protoc-25.0-rc-1-linux-x86_64.zip\",\n",
    "    'tensorflow_models': \"https://github.com/tensorflow/models/archive/refs/heads/master.zip\",\n",
    "    'pretrained_model': \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\",\n",
    "    'tfrecord_script': \"https://github.com/afaqueumer/TensorFlow-Object-Detection/raw/main/generate_tfrecord.zip\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/bzy/TensorFlow\"\n",
    "new_model_name = 'custom_cube_detect' \n",
    "pretrained_model = 'ssd_mobilenet_v2_320x320_coco17_tpu-8'\n",
    "tf_record_script = 'generate_tfrecord.py'\n",
    "label_map = 'label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'tensorflow': root_path,    \n",
    "    'workspace': os.path.join(root_path, 'workspace'),\n",
    "    'scripts': os.path.join(root_path,'scripts'),\n",
    "    'api': os.path.join(root_path,'models'),\n",
    "    'annotation': os.path.join(root_path, 'workspace','annotations'),\n",
    "    'image': os.path.join(root_path, 'workspace','images'),\n",
    "    'train_images': os.path.join(root_path, 'workspace','images','train'),\n",
    "    'test_images': os.path.join(root_path, 'workspace','images','test'),\n",
    "    'model': os.path.join(root_path, 'workspace','models'),\n",
    "    'pretrained_model': os.path.join(root_path, 'workspace','pre-trained-models'),\n",
    "    'checkpoint': os.path.join(root_path, 'workspace','models', new_model_name), \n",
    "    'output': os.path.join(root_path, 'workspace','models', new_model_name, 'export'), \n",
    "    'protoc':os.path.join(root_path,'protoc')\n",
    " }\n",
    "\n",
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'train_pipeline':os.path.join('/home/bzy/tf', 'workspace','models', new_model_name, 'pipeline.config'),\n",
    "    'tf_record_script': os.path.join(paths['scripts'], tf_record_script), \n",
    "    'labelsmap': os.path.join(paths['annotation'], label_map)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(links[\"protobuff\"], paths['protoc'])\n",
    "!cd {paths['protoc']} && unzip \"protoc*.zip\"\n",
    "\n",
    "os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['protoc'], 'bin')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(links[\"tensorflow_models\"], paths['api'])\n",
    "!cd {paths['api']} && unzip models-master.zip\n",
    "!cd {paths['api']} && cp -r ./models-master/. ./.\n",
    "!cd {paths['api']} && rm -rf models-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(links[\"pretrained_model\"], paths['pretrained_model'])\n",
    "!cd {paths['pretrained_model']} && tar -zxvf {pretrained_model + '.tar.gz'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wget.download(links[\"tfrecord_script\"], paths['scripts'])\n",
    "!cd {paths['scripts']} && unzip generate_tfrecord.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection = os.path.join(paths['api'], 'research', 'object_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {os.path.join(paths['api'], 'research')} && protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {object_detection} && rm -rf cocoapi && git clone https://github.com/cocodataset/cocoapi.git\n",
    "!cd {object_detection}/cocoapi/PythonAPI && make\n",
    "!cd {object_detection}/cocoapi/PythonAPI && cp -r pycocotools {paths['api']}/research/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {os.path.join(paths['api'], 'research')} && cp object_detection/packages/tf2/setup.py .\n",
    "!cd {os.path.join(paths['api'], 'research')} && python -m pip install .\n",
    "\n",
    "\n",
    "# !cd {os.path.join(paths['api'], 'research')} && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py setup.py && python setup.py build && python setup.py install\n",
    "# !cd {os.path.join(paths['api'], 'research')}/slim && pip install -e \n",
    "verification_script = os.path.join(paths['api'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "!python {verification_script}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [{'id':1, 'name':'cube'}, {'id':2, 'name':'cube'}]\n",
    "with open(files['labelsmap'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write(f'\\tid:{label[\"id\"]}\\n')\n",
    "        f.write(f'\\tname:\\'{label[\"name\"]}\\'\\n')\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {files['tf_record_script']} -x {paths['train_images']} -l {files['labelsmap']} -o {os.path.join(paths['annotation'], 'train.record')} \n",
    "!python {files['tf_record_script']} -x {paths['test_images']} -l {files['labelsmap']} -o {os.path.join(paths['annotation'], 'test.record')} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -i {os.path.join(paths['pretrained_model'], pretrained_model, 'pipeline.config')} {os.path.join(paths['checkpoint'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -i {os.path.join(paths['api'], 'research', 'object_detection', 'model_main_tf2.py')} {paths['scripts']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_script = 'model_main_tf2.py'\n",
    "# training_command = \"python {} --model_dir=/models/{} --pipeline_config=models/{}/pipeline.config\".format(training_script, new_model_name, new_model_name)\n",
    "training_command = f\"cd {paths['workspace']} && python model_main_tf2.py --model_dir=models/{new_model_name} --pipeline_config_path=models/{new_model_name}/pipeline.config\"\n",
    "! pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!{training_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating_command = f\"cd {paths['workspace']} && python model_main_tf2.py --model_dir=models/{new_model_name} --pipeline_config_path=models/{new_model_name}/pipeline.config --checkpoint_dir=models/{new_model_name}\"\n",
    "!{evaluating_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_command = f\"cd {paths['workspace']} && tensorboard --logdir=models/{new_model_name}\"\n",
    "!{monitoring_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!cp -i {os.path.join(paths['api'], 'research', 'object_detection', 'exporter_main_v2.py')} {paths['scripts']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporting_script = os.path.join(paths['scripts'], 'exporter_main_v2.py')\n",
    "exporting_command = \"python {} --input_type image_tensor --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(exporting_script, files['train_pipeline'], paths['checkpoint'], paths['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{exporting_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
